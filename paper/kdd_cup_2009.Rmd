---
title: "KDD Cup 2009 - Customer Relationship Prediction"
author: "by Udy Akpan, Joe Dion, Sandra Duenas, Manjari Srivastava, Jay Swinney"
date: "Summer Quarter 2015"
output: 
  pdf_document: 
    number_sections: yes
---

\centering
\raggedright
\newpage
\tableofcontents



\pagebreak
# Summary

# Introduction

Customer Relationship Management (CRM) software first became available in the 90s and has proliferated through companies large and small as a way to track interactions among companies and their customers.  Whether you call Citibank, Verizon, Comcast, Carnival Cruise Lines about that cruise you are planning or Microsoft Technical support to fix a technology problem, or you are the target of an email marketing compaign, it is highly likely that your interaction will be captured in a CRM system.  While the original purpose of CRM systems was to track customer interactions to closure, over time the data about customer interests, preferences and actions have become increasingly valuable and more effort is being made to extract insight to improve business decisions.     

The goal of this modeling exercise is exactly that, the analysis of customer data in a CRM database with an eye towards building models to predict future customer actions. The dataset in question, the KDD Cup 2009 CRM problem is a dataset from Orange, a French Telecom company that was used as part of a KDD competition and consists of 50,000 observations with 230 variables, 190 of which are numerical and 40 of which are categorical.  There are three target variables that are subject to prediction and these variables are binary, marked with either a 1 incidating the outcome occurred or exists for that observation or a 0 indicating that the outcome did not occur or exist for that observation.  The target variables of Churn, Appetency and Upselling as described below; There is no overlap among the three variables, i.e. if a customer has a 1 for churn, they will have 0s for both of the other variables.

Churn: Churn might also be thought of as attribution and in the dataset it is assumed that a 1 value indicates that a customer has stopped using the company's services. Out of the 50,000 observations only 7% have a 1 for churn. 

Appetency: Represents the customers willingness to buy the service. It is assumed that a 1 value indicates a customer is likely willing to use additional services. Only 2% of observations are marked with a 1 for appetency indicating a proclivity for buying.   

Upselling: Represents the likelihood of the customer to upgrade to a more profitable services. It is assumed that a 1 value indicates that a customer is likely to upgrade or be subject to an upselling marketing approach.  About 7% of the observations have a 1 indicating upselling.

#Issues

There are a significant number of issues with the dataset that required solutions before model development could begin.

Number of Variables: As noted above, there are 230 variables in this dataset and therefore building model to required an approach to variable selection and reduction that would produce the most effective collection of variables.

Anonymity:  Several levels of anonymity have been implemented. First the variable names have been replaced by number values, i.e. Var1 to Var290. Secondly, the variable values have been replaced with seemingly non-sensical information for instance categorical vriables have been replaced with series of random characters.  The random series of characters are present in more than one observation so clearly represent some type of categorical identification, but, it's not clear what that is. The actual product or service that the company is offering is also unknown.

Unknown Granularity: There are 50,000 observations in the dataset, however, it is not clear if each observation represents one customer or for instance if observations are targets of marketing campaigns where a single customer can appear more than once. For model building it is assumed that each observation represents a single customer.  

Missing Variables: Many of the observations are missing values for many of the variables. Combined with the anonymity above, it's difficult to determine if data is missing for a legitimate reason, as in possibly the values represent marketing campaigns and a missing value indicates that that customer was not targeted by that campaign.

#Missing Variable Resolution

Given that the predictors are unknown and are generically labeled, a strategy was developed for data imputation.

Numeric variables: Missing values for numeric variable was done by using zero (0) for numeric variables

Categorical variables: Missing values for categorizal variables were replaced with 'missing' and a indicator variable was added to retain visibility with a 1 indicating the value was replaced and a 0 indicating it was not.  
  
# The Modeling Problem  

The goal is to identify the most effective set of variables and most effective model or combinations of models to predict a future customer's likelihood of churn, appetency or upselling using the avaialbe data. As each variable has a yes or no outcome the models used will be those applicable to binary classification outcomes such as logistic, Support Vector Machines, and Decision Trees.  Each of the target variables have been considered independently and the variable selection process was applied to each target separately.    

In addition to identifying the best model for each outcome variable, as the dataset has been used in a KDD competition there is a secondary goal to exceed the result of the winning groups from that competiton. The results of those teams below, along with information on the approach used, was considered during model development. The original competition used both a large dataset consisting of 15,000 variables and a smaller dataset with 230 variables. All comparisons will be made against the smaller dataset.  


First Place: University of Melbourne (The generally satisfactory model)

```{r, echo = FALSE, results='asis'}
library(xtable)
options(xtable.comment = FALSE)
print(xtable(data.frame(Churn = 0.757,
                        Appetency = 0.8836,
                        Upselling = 0.9048,
                        Score = 0.8484), digits=4,
             caption = 'University of Melbourne'),
      include.rownames=FALSE)
```
First Runner Up: Financial Engineering Group, Inc. Japan (Stochastic Gradient Boosting)

```{r, echo = FALSE, results='asis'}
print(xtable(data.frame(Churn = 0.7589,
                        Appetency = 0.8768,
                        Upselling = 0.9074,
                        Score = 0.8477), digits=4,
             caption = 'Financial Engineering Group, Inc. Japan'),
      include.rownames=FALSE)
```

Second Runner Up: National Taiwan University, Computer Science and Information Engineering (Fast Scoring on a Large Database using regularized maximum entropy model,categorical/numerical balanced AdaBoost and selective Naive Bayes)


```{r, echo = FALSE, results='asis'}
print(xtable(data.frame(Churn = 0.7558,
                        Appetency = 0.8789,
                        Upselling = 0.9036,
                        Score = 0.8461), digits=4,
             caption = 'National Taiwan University'),
      include.rownames=FALSE)
```

DO WE REALLY NEED THIS ONE?
However, the IBM Research Submission does not appear as a Winner of the Slow Track, it has the submission Score as follows



```{r, echo = FALSE, results='asis'}
print(xtable(data.frame(Churn = 0.7651 ,
                        Appetency = 0.8819,
                        Upselling = 0.9092,
                        Score = 0.8521), digits=4,
             caption = 'IBM'),
      include.rownames=FALSE)
```

Evaluation:  The results of the overall modeling exercise will be evaluated according to the arithmetic mean of the AUC for the three prediction tasks (churn, appetency. and up-selling). This is considered the "Score".  Larger numerical values indicate higher confidence that observations in the test set are correctly classified. The goal is to exceed the results of the in-house model which are shown below.  The winning competitors from the KDD competition above only slightly beat the in-house model.

```{r, echo = FALSE, results='asis'}
print(xtable(data.frame(Churn =  0.7435 ,
                        Appetency = 0.8522,
                        Upselling = 0.8975,
                        Score = 0.83107), digits=4,
             caption = 'In House Models'),
      include.rownames=FALSE)
```

# The Data


```{r, echo = FALSE, results='asis', message = FALSE, warning=FALSE}
if(dir.exists('c:/Users/jay')){
  setwd('c:/Users/jay/Dropbox/pred_454_team')
}else if(dir.exists('c:/Users/uduak')){
  setwd('c:/Users/uduak/Dropbox/pred_454_team')
}
# get the response variables
churn_ <- read.csv('data/orange_small_train_churn.labels', header = FALSE)
appetency_ <- read.csv('data/orange_small_train_appetency.labels',
                       header = FALSE)
upsell_ <- read.csv('data/orange_small_train_upselling.labels',
                    header = FALSE)

# change -1 to 0
churn_[churn_$V1 < 0,] <- 0
appetency_[appetency_$V1 < 0,] <- 0
upsell_[upsell_$V1 < 0,] <- 0

colnames(churn_) <- c('churn')
colnames(appetency_) <- c('appetency')
colnames(upsell_) <- c('upsell')

# add response variables to the data
df <- cbind(churn_, appetency_, upsell_)

library(fBasics)
library(dplyr)
library(xtable)

df <- dplyr::select(data.frame(t(basicStats(df))), 1:9)
colnames(df)[c(5,6,9)] <- c('Q1', 'Q2','Positive_Instances')
print(xtable(data.frame(df), caption = 'Response Variables'))

```
For the original competition, 2 data sets were used by competitiors, a large dataset consisting of 15,000 variables and a reduced data set of only 230 variables available for competitiors using personal computers rather than larger more powerful systems. As previously noted, the dataset has a number of issues.  The variable names have been replace with generic names, i.e. Var1, Var2, etc, so, for each variable, there is no way to determine what the variable actually represents.  There are many missing values, but imputation of missing values is even more difficult than normal as knowledge about what a variable represents could aid in selecting the imputation method.

The table below shows that there are no missing values in the target variables, values are either a 1 or a 0  and a small number of the outcome variables have 1s, indicating the status for that observation, so, for Churn, a 1 indicates that the customer churned or left, for appetency, the 1 indicates a propensity to buy a product or service and for upselling a 1 indicates the customer has acquired additional products or services or has upgraded their products or services.   

\pagebreak

# Exploratory Data Analysis

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(dev = 'pdf')
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(lattice)
library(plyr)
library(dplyr)
library(tidyr)
library(grid)
library(gridExtra)
library(ROCR)
library(e1071)
library(knitr)
library(ggplot2)
library(data.table)
library(glmnet)
library(randomForest)
library(rpart)
library(rpart.plot)
library(rattle)

# load pre-trained models
load('c:/Users/jay/Dropbox/pred_454_team/eda.RData')

# set document width
# read in the data to R
# I'm using na.stings = '' to replace blanks with na
# this also helps R read the numerical varaibles as numerical
setwd('c:/Users/jay/Dropbox/pred_454_team')
df <- read.csv('data/orange_small_train.data', header = TRUE,
               sep = '\t', na.strings = '')
# read the target variables
churn_ <- read.csv('data/orange_small_train_churn.labels', header = FALSE)
appetency_ <- read.csv('data/orange_small_train_appetency.labels', header = FALSE)
upsell_ <- read.csv('data/orange_small_train_upselling.labels', header = FALSE)

churn_[churn_$V1 < 0,] <- 0
appetency_[appetency_$V1 < 0,] <- 0
upsell_[upsell_$V1 < 0,] <- 0
```
## Imputing Missing Data

The most frequent strategy used to impute missing data was to replace missing numeric values with a 0 and create a new binary variable indicating that the variable in the original dataset was missing. For categorical variables, all classes that represent less than 1% of the total observations were grouped into an "other" category, then a separate missing class was created. The categorical variables were replaced with the word 'missing' and a new variable was added with an indicator value set to 1 to indicate that the variable was imputed or to 0 to indicate no imputation.  Through this method the original variables include values for all observations and separately, a variable with missing values can be used for modeling as well in the case that the variable being present or absent is predictive. 

```{r, message=FALSE, warning=FALSE, echo=FALSE}

# impute mising data with zeros and "missing"
# also creates missing variable column
for (i in names(df)){
  vclass <- class(df[,i])
  if(vclass == 'logical'){
    # some of the variables are 100% missing, they are the only logical class vars
    # so we can safely remove all logical class vars
    df[,i] <- NULL
  }else if(vclass %in% c('integer', 'numeric')){
    #first check that there are missing variables
    if(sum(is.na(df[,i])) == 0) next
    # create a missing variable column
    df[,paste(i,'_missing',sep='')] <- as.integer(is.na(df[,i]))
    # fill missing variables with 0
    df[is.na(df[,i]),i] <- 0
  }else{
    # gather infrequent levels into 'other'
    levels(df[,i])[xtabs(~df[,i])/dim(df)[1] < 0.015] <- 'other'
    # replace NA with 'missing'
    levels(df[,i]) <- append(levels(df[,i]), 'missing')
    df[is.na(df[,i]), i] <- 'missing'
  }
}

# add the target variables to the data frame
df$churn <- churn_$V1
df$appetency <- appetency_$V1
df$upsell <- upsell_$V1
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# get the index for training/testing data
set.seed(123)
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
# making a "tiny" data set so I cn quickly test r markdown and graphical paramters
# this will be removed in the submitted version
tiny_ind <- sample(seq_len(nrow(df)), size = floor(0.01 * nrow(df)))
# split the data
train <- df[train_ind, ]
test <- df[-train_ind, ]
tiny <- df[tiny_ind, ]

df_mat <- select(df, -churn, -appetency, -upsell)

for (i in names(df_mat)){
  if (class(df_mat[,i]) == 'factor'){
    for(level in unique(df_mat[,i])){
      df_mat[sprintf('%s_dummy_%s', i, level)] <- ifelse(df_mat[,i] == level, 1, 0)
    }
    df_mat[,i] <- NULL
  } else {
    # scale numeric variables
    # this is important for regularized logistic regression and KNN
    df_mat[,i] <- scale(df_mat[,i])
  }
}

df_mat <- data.matrix(df_mat)
```

##Variable Selection

As noted previously, with 230 original variables, plus additional dummy variables to represent missing values, and variables with anonymized information, a method was needed to reduce the set of variables to be used in modeling.  Logistic models with Elastic Net Penalty, Decision Trees and Random Forest were run for each of the target variables to identify the most viable variables.


## Appetency

The first response variable to discuss is appetency. As defined in the task description on the KDD website, appetency is the propensity to buy a service or a product. Only 2% of observations have a positive inticator for appetency.


### Logistic Regression with Elastic-Net Penalty

LETS ADD A TABLE WITH A COMPARISON OF THE VARIABLES SELECTED ACROSS THE THREE METHODS
The results from the logistic regression shown below are very promising.  The AUC peaks above 0.8 and does not dramatically decline until nearly all of the variables are removed from the model. This shows that a small number of variables are going to be strong indicators of appetency.

```{r lreg_app, echo=FALSE, fig.width=5, fig.height=3, fig.align='center'}
# view the AUC of differnt values of lambda
plot_df <- data.frame(cvm = app_lreg.cv$cvm, cvup = app_lreg.cv$cvup,
                      cvlo = app_lreg.cv$cvlo, lambda = app_lreg.cv$lambda)

ggplot(data = plot_df, aes(x = log(lambda), y = cvm)) +
  geom_line(colour = '#3b5b92', size = 1) +
  geom_ribbon(aes(x = log(lambda), ymin = cvlo, ymax = cvup),
              alpha=0.2, fill = '#3b5b92') +
  geom_vline(xintercept = log(app_lreg.cv$lambda.min), linetype = 3, size = 1) +
  geom_vline(xintercept = log(app_lreg.cv$lambda.1se), linetype = 3, size = 1) +
  ylab('AUC') + ggtitle('Cross Validation Curve Logistic Regression')


```


The table below indicates that with just 3 variables in the highly regularized model (right-most vertical line) Var126 and a couple of levels of dummy variable for Var218 are very indicative of appetency, meaning that predicting appetency should be relatively easy.

```{r kable_app, results= 'asis', echo=FALSE, message=FALSE, warning=FALSE}
cv_coefs <- data.frame(
  coeficient = coef(app_lreg.cv, s = 'lambda.1se')[abs(coef(app_lreg.cv,
                                                        s = 'lambda.1se')) > 1e-3])

row.names(cv_coefs) <- row.names(coef(app_lreg.cv,
   s = 'lambda.1se'))[abs(as.vector(coef(app_lreg.cv, s = 'lambda.1se'))) > 1e-3]

kable(cv_coefs, caption = "Variables Selected by Elastic-Net")
```


```{r lreg_app_perf, echo=FALSE, fig.width=5, fig.height=4, fig.align='center'}
yhat <- predict(app_lreg.cv, df_mat[-train_ind,], type = 'response')

pred <- prediction(yhat, factor(test$appetency))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")

p <- ggplot(data.frame(TPR = unlist(perf@y.values),
                       FPR = unlist(perf@x.values)),
            aes(FPR, TPR)) + geom_line(color = '#3b5b92', size = 1) +
  xlab('False Positvie Rate') + ylab('True Positive Rate') +
  ggtitle('Logistic Regression ROC Curve') +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  annotate("text", x = 0.75, y = 0.20, label = sprintf('AUC: %f',
  attributes(performance(pred, 'auc'))$y.values[[1]]))
p
```

TYPO IN FALSE POSITIVE RATE (Positvie)

The ROC curve below is constructed on out of sample data, showing that the logistic regression model performs very well for appetency.


### Decision Tree

```{r dt_app, echo=FALSE, fig.width=7, fig.height=3.5, fig.align='center'}
fancyRpartPlot(app_tree, main = 'Appetency Decision Tree', sub = NULL)
#app_tree
```

The Decision Tree classifier selected 7 variables as the most predictive and they are listed below in order of predictive ability.    

1.	Var126  5.	Var206  
2.	Var218  6.	Var223  
3.	Var204  7.	Var81 
4.	Var38 

The decision tree was configured as follows: minsplit=40 to set the minimum number of observations per node, minbucket=10 to set the minimum number of total nodes, and cp=0.001 to set the cost complexity factor with a split that must decrease the overall lack of fit by a factor of 0.001.  

### Random Forest

The Variable Importance plot below for a Random Forest model identified variable 204 & 126 as two of the top three most important variables. 126 shows up in all three models and 204 also shows up in the Decision Tree.

```{r rf_app, echo=FALSE, fig.width=7, fig.height=4, fig.align='center'}
plot_df = data.frame(app_rf$importance)
plot_df$Variable <- factor(row.names(plot_df))
plot_df$Variable <- factor(plot_df$Variable,
                           levels = plot_df$Variable[order(plot_df$MeanDecreaseGini)])

plot_df <- plot_df[plot_df$MeanDecreaseGini > 12,]


ggplot(data = plot_df, aes(x = Variable,
                           y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = '#3b5b92') + coord_flip() +
  theme(axis.text.y = element_text(colour = 'black')) +
  ggtitle('Variable Importance Appetency')
```

Overall, Random Forest does not perform nearly as well as the regularized logistic regression, as the model is severely over-fit, and will need significant tuning before it reaches the level of the regularized logistic regression model.

WHAT CAN WE PUT IN TO SHOW IT'S OVERFIT; ADD THE ROC CURVE FOR RANDOM FOREST


```{r rf_app_perf, echo=FALSE, fig.width=3, fig.height=3, fig.align='center', eval = FALSE}
yhat <- predict(app_rf, select(test, -churn, -upsell), type = 'prob')

pred <- prediction(yhat[,2], factor(test$appetency))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
p <- ggplot(data.frame(TPR = unlist(perf@y.values),
                       FPR = unlist(perf@x.values)),
            aes(FPR, TPR)) + geom_line(color = '#3b5b92', size = 1) +
  xlab('False Positvie Rate') + ylab('True Positive Rate') +
  ggtitle(' Random Forest ROC Curve') +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  annotate("text", x = 0.75, y = 0.20, label = sprintf('AUC: %f',
  attributes(performance(pred, 'auc'))$y.values[[1]]))
p
```

### K-Nearest Neighbors & Naïve Bayes

IS THERE A LIST OF VARIABLES IDENTIFIED HERE?

The variable selection process for appetency was based on the smallest deviance of each variable.  This variable selection process resulted in 31 variables out of 230 with deviance of 186.294 or less based on the Calibration data set.  The Calibration data set is a 10% random selection of observations from the original data set.  

###comparison of Variables Identified by Each Method

WILL ADD A TABLE WITH THE VARIABLES EACH METHOD SELECTED 


## Churn

The next target variable to be evaluated is Churn, which is the likelihood that a customer will discontinue using the goods and services of the company.  7% of observations have a value indicating that the customer has churned. As with appetency, logistic regression with an elastic net penalty, a Decision Tree and Random Forest model were all applied in order to select the most useful variables.

### Logistic Regression with Elastic-Net Penalty

Relative to appetency, the Logistic Regression model did not perform nearly as well with an AUC score in the mid 70s compared to 80 for the appetency model. Furthermore, the regularized and cross validated model selected 155 variables, many of which are the dummy variables created to indicate that variables were missing. 


```{r lreg_churn, echo=FALSE, fig.width=5, fig.height=3, fig.align='center'}
# view the Area Under the Curve for different values of lambda.
plot_df <- data.frame(cvm = churn_lreg.cv$cvm, cvup = churn_lreg.cv$cvup,
                      cvlo = churn_lreg.cv$cvlo, lambda = churn_lreg.cv$lambda)

ggplot(data = plot_df, aes(x = log(lambda), y = cvm)) +
  geom_line(colour = '#d9544d', size = 1) +
  geom_ribbon(aes(x = log(lambda), ymin = cvlo, ymax = cvup),
              alpha=0.2, fill = '#d9544d') +
  geom_vline(xintercept = log(churn_lreg.cv$lambda.min), linetype = 3, size = 1) +
  geom_vline(xintercept = log(churn_lreg.cv$lambda.1se), linetype = 3, size = 1) +
  ylab('AUC') + ggtitle('Cross Validation Curve Logistic Regression')

```

For brevity, a small subset of the variables selected are displayed below with their coefficients, note that most of the variables selected are the dummy variables indicating that a value is present or absent.

```{r kable_churn, results= 'asis', echo=FALSE, message=FALSE, warning=FALSE}
cv_coefs <- data.table(variable = row.names(coef(churn_lreg.cv))[
                          abs(as.vector(coef(churn_lreg.cv))) > 1e-5],
                       coeficient = coef(churn_lreg.cv)[abs(coef(churn_lreg.cv)) > 1e-5])


kable(cv_coefs[variable %like% '26'],
      caption = "Variables Selected by Elastic-Net")
```



### Decision Tree

The Decision Tree model for churn is shown below. Of particular importance, variables 126 and 226 show up in more than one split and these variables were also identified as important in the logistic model above.

```{r dt_churn, echo=FALSE, fig.width=7, fig.height=3.5, fig.align='center'}
fancyRpartPlot(churn_tree, main = 'Churn Decision Tree', sub = NULL)
```


### Random Forest

While the Random Forest model performs very poorly, indicating and ROC value of just over .6, by looking at the Variable Importance chart, Var226 and Var126 show up again, indicating that these are likely to be highly predictive variables.

```{r rf_churn,echo=FALSE, fig.width=7, fig.height=3.5, fig.align='center'}
plot_df = data.frame(churn_rf$importance)
plot_df$Variable <- factor(row.names(plot_df))
plot_df$Variable <- factor(plot_df$Variable,
                           levels = plot_df$Variable[order(plot_df$MeanDecreaseGini)])

plot_df <- plot_df[plot_df$MeanDecreaseGini > 50,]

ggplot(data = plot_df, aes(x = Variable, y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = '#d9544d') + coord_flip() +
  theme(axis.text.y = element_text(colour = 'black')) +
  ggtitle('Varible Importance Churn')
```


```{r, echo=FALSE, fig.width=3, fig.height=3, fig.align='center'}
yhat <- predict(churn_rf, select(test, -appetency, -upsell), type = 'prob')

pred <- prediction(yhat[,2], factor(test$churn))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
p <- ggplot(data.frame(TPR = unlist(perf@y.values),
                       FPR = unlist(perf@x.values)),
            aes(FPR, TPR)) + geom_line(color = '#d9544d', size = 1) +
  xlab('False Positvie Rate') + ylab('True Positive Rate') +
  ggtitle('Random Forest ROC Curve') +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  annotate("text", x = 0.75, y = 0.20, label = sprintf('AUC: %f',
  attributes(performance(pred, 'auc'))$y.values[[1]]))
p
```
The ROC curve above is on the out of sample data and performs very poorly, although the ROC curve on the in sample data, which is not displayed here, performed very well. This indicates that the model is overfit and will require additional tuning. Options include changing the requirements for leaf and split sizes and trying the random forest with a subset of variables such as the ones selected by regularized logistic regression.

### K-Nearest Neighbors & Naïve Bayes

The variable selection process for churn was based on the smallest deviance of each variable.  This variable selection process resulted in 47 variables out of 230 with deviance of 291.862 or less based on the Calibration data set.  The Calibration data set is a 10% random selection of observations from the original data set.


## Up-Sell

The last response variable to be evaluated is up-sell. Up-selling indicates that the customer has purchased additional goods and services or has upgraded to a higher level of goods and services. 7% of the observations have a positive indicator for up-sell.

### Logistic Regression with Elastic-Net Penalty
```{r lreg_upsell, echo=FALSE, fig.width=5, fig.height=3, fig.align='center'}
# view the AUC of differnt values of lambda
plot_df <- data.frame(cvm = upsell_lreg.cv$cvm, cvup = upsell_lreg.cv$cvup,
                      cvlo = upsell_lreg.cv$cvlo, lambda = upsell_lreg.cv$lambda)

ggplot(data = plot_df, aes(x = log(lambda), y = cvm)) +
  geom_line(colour = '#39ad48', size = 1) +
  geom_ribbon(aes(x = log(lambda), ymin = cvlo, ymax = cvup),
              alpha=0.2, fill = '#39ad48') +
  geom_vline(xintercept = log(upsell_lreg.cv$lambda.min), linetype = 3, size = 1) +
  geom_vline(xintercept = log(upsell_lreg.cv$lambda.1se), linetype = 3, size = 1) +
  ylab('AUC') + ggtitle('Cross Validation Curve Logistic Regression')
```
The results from the regularized logistic regression show an AUC score comparable to appetecy in the .80 range, however with 80 remaining variables in the model. Regularization does not appear to yield much performance gain.


```{r lreg_upsell_coefs, eval=FALSE, echo=FALSE}
cv_coefs <- data.frame( coeficient = coef(upsell_lreg.cv, s = 'lambda.1se')[
  abs(coef(upsell_lreg.cv, s = 'lambda.1se')) > 1e-3])

row.names(cv_coefs) <- row.names(coef(upsell_lreg.cv, s = 'lambda.1se'))[
  abs(as.vector(coef(upsell_lreg.cv, s = 'lambda.1se'))) > 1e-3]
```

### Decision Tree
```{r dt_upsell, echo=FALSE, fig.width=8, fig.height=4, fig.align='center'}
fancyRpartPlot(upsell_tree, main = 'Up-Sell Decision Tree', sub = NULL)
```

The Decision Tree identified Var126 and Var28 as having high importance.These variables likely have good predictive value for up-sell. Control options used for the decision tree include: minsplit set the minimum number of observations per node, minbucket to minimum number of total nodes , cp - split must decrease the overall lack of fit by a factor of 0.001 (cost complexity factor). 

```{r, eval=FALSE, echo=FALSE}
summary(upsell_tree)
```

### Random Forest

The Random Forest classifier selected nearly 200 predictor variables as having significant predictive value for up-sell, which does not help in reducing the number of variables for modeling. The Variable Importance plot below does includ Var126 which shows as an important variable in the Decision tree.
```{r rf_upsell, echo=FALSE, fig.width=7, fig.height=4, fig.align='center'}
plot_df = data.frame(churn_rf$importance)
plot_df$Variable <- factor(row.names(plot_df))
plot_df$Variable <- factor(plot_df$Variable,
                           levels = plot_df$Variable[order(plot_df$MeanDecreaseGini)])

plot_df <- plot_df[plot_df$MeanDecreaseGini > 50,]
p <-  ggplot(data = plot_df, aes(x = Variable,
                           y = MeanDecreaseGini)) +
        geom_bar(stat = "identity", fill = '#39ad48') + coord_flip() +
        theme(axis.text.y = element_text(colour = 'black')) +
        ggtitle('Variable Importance Up-Sell')
p
```



### K-Nearest Neighbors & Naïve Bayes

The variable selection process for up-sell was based on the smallest deviance of each variable.  This variable selection process resulted in 51 variables out of 230 with deviance of 504.483 or less based on the Calibration data set.  The Calibration data set is a 10% random selection of observations from the original data set.  
  
  


# Predictive Modeling: Methods and Results
```{r, echo = FALSE}
knitr::read_chunk('script_for_paper.R')
```
```{r rdata,echo = FALSE}
```
```{r combine predictions,echo = FALSE}
```

## Train/Test Data

The following code was used for every model to ensure that the same testing/training data was used. We chose to use a 75/25 split for training and testing data sets.

```{r, eval=FALSE}
# set seed to make reporducable results
set.seed(123)
# get train/test indicies
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
# split the data
train <- df[train_ind, ]
test <- df[-train_ind, ]
```

## Appetency

### Naïve Bayes
The Naïve Bayes technique was applied in a computational EDA manner to obtain the highest AUC score for appetency.

The variable selection process was based on the smallest deviance of each variable.  This variable selection process resulted in 31 variables out of 230 with deviance of 186.294 based on the Calibration data set.  

The Calibration data set is a 10% random selection of observations from the original data set.

The resulting Naive Bayes model using the selected variables shows that the model is over fitting the data because the AUC Score with the Train data is 0.9619 but the AUC Score with the Test data is 0.7624, which is about a 20-point difference.  However, the AUC for the Test is significantly above 0.50 of a random guess, so we could consider the Naive Bayes model for appetency to be reasonably accurate.

### Random Forest  
The Random Forest classifier was used to build a classification model for Appetency variable on the training data set. The parameters chosen were number of trees = 50 and minimum bucket size =10 . The first model was built choosing all the 230 variables and the imputed variables. This resulted in a random forest model with over 200 variables. This model however was not able to detect appetency=1 cases in the test data set  in spite of a decent accuracy level of 98.6% in test data set.  

Multiple models were created using a subset of variables based on importance - top 25 and top 50 variables. These models did have a high accuracy but were not able to detect appetency=1 cases in test data set.
Also models were created using the sample size option (sampsize = c(10,30)) , this implies that the algorithm will randomly draw 10 and 30 from two values of appetency = 0 and 1 to grow the tree. This improved the accuracy of the model, but appetency =1 customers were still not predicted correctly. Random forest may not be a good classifier for appetency.  

Area under the curve (AUC) is used to determine goodness of fit for the model. AUC for Random forest model for Appetency in test data is 0.622. This is a low value, so the model is only slightly better than a random pick. 

#### Random Forest 2
A second approach to random forest was attempted because online research indicated that it was a powerful algorithm for this competition. Before fitting the random forest, each positive record of appetency in the training set was copied 3 times, so that there would now be 4 copies of each positive record. This oversampling of positive appetency cases was intended to make the random forest model predict 1 for appetency more often by changing the ratio of positive and negative cases in the data. All variables were used and like many previous models missing observations were replaced with 0 and a binary variable was created to indicate missing values.


### Logistic Regression

Logistic regression with a LASSO shrinkage was used to predict appetency. All variables were included in the model with no special transformations beyond our standard imputation of 0's, creating binary variables for missing indication and grouping the less frequent classes of qualitative variables. The value of lambda that achieved the maximum AUC on cross validated training data was used to make predictions for the test data set.

#### Invesitigative Varaible Selection

In addition to just allowing LASSO to select variables for logistic regression, some significant work was put into selecting variables in a more thoughtful manner with the hope that it would provide a better AUC. The process is described below.

#### Decision Tree Variable selection

The next response variable to discuss is appetency. As defined in the task description on the KDD website, appetency is the propensity to buy a service or a product. Fitting a naive decision tree on the training data set produces a tree constructed using minsplit (the minimum number of observations that must exist in a node in order for a split) and minbucket (the minimum number of observations in any terminal <leaf> node) is set to the values 100 and 10 respectively. We obtain that the following variables six are interesting with regards to appetency: Var126, Var204_dummy_RVjC, Var218_dummy_cJvF, Var25, Var38, and Var57.    
A graphical analysis (not shown) of these 6 variables reveals the following:  
1)	Lower values of Var126( between -25 and +13) seem to be associated with high proportionate appetency  
2)	A higher count of appetency for people with no values for Var204_dummy_RVjC  
3)	A higher count of appetency for people with no values for Var218_dummy_cJvF  
4)	High counts of appetency for Var25 value below 2000  
5)	High counts of appetency for Var38 values below 5.0e+06  
6)	Relatively similar counts of appetency across all values of Var57. 

##### Goodness-of-fit of decision tree variables  

Using the above variables obtained from our decision tree variable selection step, we proceed to fit a logistic regression model on the entire data set using appetency as a target. We obtain that only three variables are statistically significant. An ANOVA analysis between the full model containing all 6 variables and a reduced model containing the three statistically significant variables indicates that the reduced model fits as well as the full model. The variables are Var126, Var218_dummy_cJvF and Var38. We also note a chi-square goodness of fit test for the overall model is significant at p=0.05  

#### LASSO Variable Selection   

Using the LASSO (shrinkage parameter, lambda=1) we obtain a selection of 33 variables when the shrinkage parameter, lambda, is at its minimum.   

Goodness-of-fit of LASSO variables  

Using the above variables obtained from our LASSO exploratory model, we proceed to fit a logistic regression model on the entire data set using appetency as a target. We find that several variables are not statistically significant, however, an ANOVA analysis between the full model containing all 33 variables identified by the LASSO, fit better than a reduced model in which statistically insignificant variables are dropped. We also note a chi-square goodness of fit test for the overall model is significant at p=0.05.  

##### GOF on LASSO variables   

Using the above variables obtained from our LASSO exploratory model, we proceed to fit a logistic regression model on the entire data set using appetency as a target. We find that 2 variables are inestimable and several variables are not statistically significant, however, an ANOVA analysis between the full model containing 30-2(28) variables identified by the random forest model, fit better than a reduced model in which statistically insignificant variables are dropped. We also note a chi-square goodness of fit test for the overall model is significant at p=0.05.  

### K-Nearest Neighbors

The Nearest Neighbor K technique was applied in a computational EDA manner to obtain the highest AUC score for appetency.

The variable selection process was based on the smallest deviance of each variable.  This variable selection process resulted in 31 variables out of 230 with deviance of 504.483 based on the Calibration data set.

The Calibration data set is a 10% random selection of observations from the original data set.  

The resulting knn model used the selected variables and k = 200.  It shows that the model is over fitting the data because the AUC Score with the Train data is 0.9904 but the AUC Score with the Test data is 0.6548, which is about a 33-point difference.  The AUC for the Test is somewhat significantly above 0.50 of a random guess, so we could consider the knn model for up-sell to be reasonably accurate. 

### Support Vector Machine

The variable selection process started with the variables selected by the random forest as being the strongest and then do to performance issues, was reduced to only the 5 strongest variables.

Training the model was also taking as long as an hour for the training data set as well as even a small subset of variables, so, a separate data set of 20% of the observations was created for the SVM model. 

When the results were applied to the test data set, the ROC curve shows results that don't differ significantly from a random model. The model herein relies on the radial kernel with a cost of 10. linear, polynomial, radial and sigmoid methods were all attempted without significant improvement.

### Model Performance
```{r app ROC, echo = FALSE, fig.width = 7, fig.height=4}
```
```{r app AUC, echo = FALSE, results = 'asis'}
```

## Churn

### Naïve Bayes

The Naïve Bayes technique was applied in a computational EDA manner to obtain the highest AUC score for churn.

The variable selection process was based on the smallest deviance of each variable.  This variable selection process resulted in 47 variables out of 230 with deviance of 291.862 based on the Calibration data set.  

The Calibration data set is a 10% random selection of observations from the original data set.

The resulting Naive Bayes model using the selected variables shows that the model is over fitting the data because the AUC Score with the Train data is 0.9315 but the AUC Score with the Test data is 0.6622, which is about a 27-point difference.  However, the AUC for the Test is significantly above 0.50 of a random guess, so we could consider the Naive Bayes model for churn to be reasonably accurate.

### Random Forest

The Random Forest classifier was used to build a classification model for Churn variable on the training data set. The parameters chosen were, number of trees = 50 and minimum bucket size =10 . The first model was built choosing all the 230 variables and the imputed variables. This resulted in a random forest model with over 200 variables. This model however was not able to detect churn=1 cases in the test data set  in spite of a decent accuracy level of 92.3% in test data set.  

The model was then refined using the top 50 variables based on importance from the first model. This model showed higher accuracy percentage (92.4%). The model was able to detect churn=1 scenarios better than the previous random forest model.
Area under the curve (AUC) is used to determine goodness of fit for the model. AUC for Random forest model for Churn in test data is 0.6883. This is a low value, so the model is better than a random pick but it is not a good model.  

#### Random Forest 2

Because of the success of the second random forest attempt for detecting appetency, a similar technique was employed for detecting churn. All of the positive instances of churn were over sampled by a factor of four. This did increase the AUC for random forest, but the result was not as dramatic as it was for appetency.


### K-Nearest Neighbors

The Nearest Neighbor K technique was applied in a computational EDA manner to obtain the highest AUC score for churn.  

The variable selection process was based on the smallest deviance of each variable.  This variable selection process resulted in 47 variables out of 230 with deviance of 504.483 based on the Calibration data set.

The Calibration data set is a 10% random selection of observations from the original data set.  

The resulting knn model used the selected variables and k = 200.  It shows that the model is over fitting the data because the AUC Score with the Train data is 0.9801 but the AUC Score with the Test data is 0.5751, which is about a 30-point difference.  The AUC for the Test is *not* significantly above 0.50 of a random guess, so we could not consider the knn model for up-sell to be reasonably accurate.  

### Support Vector Machine

The variable selection process started with the variables selected by the random forest as being the strongest and then do to performance issues, was reduced to only the 5 strongest variables.  

Training the model was also taking as long as an hour for the training data set as well as even a small subset of variables, so, a separate data set of 20% of the observations was created for the SVM model.   

When the results were applied to the test data set, the ROC curve shows results that don't differ significantly from a random model. The model herein relies on the radial kernel with a cost of 10. linear, polynomial, radial and sigmoid methods were all attempted without significant improvement.  

### Logistic Regression

#### Tree Variable Selection
We started out with variables that were selected by a decision tree and proceeded to fit a logistic regression model using churn as a target. We obtain the following summary output of the model fit. We note that 12 variables have NA, which means they are inestimable. We will drop them from further consideration. We also note that a number of variables are not statistically significant. These we will also drop as well.
Based on these drops, the following 9 variables are considered useful from our fitted logit regression: Var126, Var217_dummy_missing, Var211_dummy_L84s, Var73, Var126_missing, Var229_dummy_missing, Var113, Var22_missing, and Var65. 
Using the chi-square goodness of fit test we obtain a p-value=0.2439 and fail to reject the null hypothesis that this model is exactly correct.

#### LASSO Variable Selection
A listing of the variables admitted into the model at log (lambda), one standard error from the minimum are: Var7, Var73, Var113, Var126, Var22_missing, Var28_missing, Var126_missing, Var205_dummy_sJzTlal, Var206_dummy_IYzP, Var210_dummy_g5HH, Var212_dummy_NhsEn4L, Var217_dummy_other, Var218_dummy_cJvF, Var218_dummy_missing, and Var229_dummy_missing.

#### GOF on LASSO variables
Using the above variables obtained from our LASSO exploratory model, we proceed to fit a logistic regression model using churn as a target. We obtain the following summary output of the model fit. We note that 1 variable has NA, which means it is inestimable. We will drop it from further consideration. We also note that a few variables are not statistically significant. These we will also drop as well.
Based on these drops, the following 10 variables are considered useful from our fitted logit regression: Var7, Var73, Var113, Var126, Var205_dummy_sJzTlal, Var210_dummy_g5HH, Var212_dummy_NhsEn4L, Var217_dummy_other, Var218_dummy_cJvF, and Var229_dummy_missing.
Using the chi-square goodness of fit test we obtain a p-value=0.6240 and fail to reject the null hypothesis that this model is correct.

#### Simple LASSO
In addition a simple logistic regression model with LASSO shrinkage was fit. This model included all of the variables and used the standard data imputation technique that other models used. The results were on the test data set show that this set up is among the most powerful algorithms for detecting churn. In the ROC curve plot, this method is identified as logistic regression 2.


### Model Performance
```{r Churn ROC, echo = FALSE, fig.width = 7, fig.height = 6}
```
```{r churn AUC, echo = FALSE, results = 'asis'}
```
\pagebreak

## Up-Sell

### Naïve Bayes
The Naïve Bayes technique was applied in a computational EDA manner to obtain the highest AUC score for up-sell.  

The variable selection process was based on the smallest deviance of each variable.  This variable selection process resulted in 51 variables out of 230 with deviance of 504.483 based on the Calibration data set.

The Calibration data set is a 10% random selection of observations from the original data set.  

The resulting Naive Bayes model using the selected variables shows that the model is over fitting the data because the AUC Score with the Train data is 0.9177 but the AUC Score with the Test data is 0.7515, which is about a 16-point difference.  However, the AUC for the Test is significantly above 0.50 of a random guess, so we could consider the Naive Bayes model for up-sell to be reasonably accurate.  


### Random Forest
The Random Forest classifier was used to build a classification model for up-sell variable on the training data set. The parameters chosen were number of trees = 50 and minimum bucket size =10 . The first model was built choosing all the 230 variables and the imputed variables. This resulted in a random forest model with over 200 variables. This model however was not able to detect up-sell=1 cases in the test data set  in spite of a decent accuracy level of 98.6% in test data set. A second model was then built using the top 25 variables based on importance from the first model. This model showed higher accuracy percentage (95.14% ) in test data set. The model was able to detect up-sell=1 scenarios better than the previous random forest model.  

Area under the curve (AUC) is used to determine goodness of fit for the model. AUC for Random forest model for up-sell in test data is 0.8334. This is a moderate value, so the model is not a perfect fit.  

### Logistic Regression
Up-sell is the easiest of the responses to detect, but has been one of the harder problems for our team to beat or get close to the in house predictions. A logistic regression model was fitted for this problem with a LASSO shrinkage parameter. Several attempts at feature engineering were made, three interaction variables were added based on the results of the decision tree discussed in the EDA portion (variable 126 and 28, variable 28 and 153 and variable 125 and 81). Also a squared version of every single numeric variable was added to the data. This created a very large data set and the model had to be trained over a period of several hours. The results showed an improvement over some other algorithms, but failed to match the performance of the random forest algorithm.

### K-Nearest Neighbors

The Nearest Neighbor K technique was applied in a computational EDA manner to obtain the highest AUC score for up-sell.  

The variable selection process was based on the smallest deviance of each variable.  This variable selection process resulted in 51 variables out of 230 with deviance of 504.483 based on the Calibration data set.

The Calibration data set is a 10% random selection of observations from the original data set.  

The resulting knn model used the selected variables and k = 200.  It shows that the model is over fitting the data because the AUC Score with the Train data is 0.9878 but the AUC Score with the Test data is 0.7021, which is about a 20-point difference.  However, the AUC for the Test is significantly above 0.50 of a random guess, so we could consider the knn model for up-sell to be reasonably accurate.

### Support Vector Machine

The variable selection process started with the variables selected by the random forest as being the strongest and then do to performance issues, was reduced to only the 4 strongest variables.  

Training the model was also taking as long as an hour for the training data set as well as even a small subset of variables, so, a separate data set of 20% of the observations was created for the SVM model.  

When the results were applied to the test data set, the ROC curve shows results that are slightly better than a random model. The model herein relies on the radial kernel with a cost of 10. linear, polynomial, radial and sigmoid methods were all attempted without significant improvement. An automate method was explored for identifying the appropriate level of C (cost) and gamma however due to the size of the data set was not successful. Cost was manually adjusted using a range from 1 indicating the narrowest margin on the hyper-plane to 10000 indicating a very wide hyper-plane with many miss-classifications.  


### Model Performance
```{r Upsell ROC, echo = FALSE, fig.width = 7, fig.height=4}
```
```{r upsell AUC, echo = FALSE, results = 'asis'}
```
\pagebreak

# Comparison of Results

# Conclusions

# Appendix

We would like to include the R code to produce the results described in this paper, but amount of code is prohibitively long. For the interested reader, all of the code used to create our results can be found at:  
https://github.com/jayswinney/454-kdd2009  



